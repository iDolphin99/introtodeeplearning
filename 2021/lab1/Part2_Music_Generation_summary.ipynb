{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoJsVjtCMunI"
   },
   "source": [
    "<table align=\"center\">\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
    "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
    "      Visit MIT Deep Learning</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning/blob/master/lab1/Part2_Music_Generation.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>\n",
    "\n",
    "# Copyright Information\n",
    "\n",
    "- Copyright 2021 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.\n",
    " \n",
    "- Licensed under the MIT License. You may not use this file except in compliance with the License. Use and/or modification of this code outside of 6.S191 must reference:\n",
    "\n",
    "- © MIT 6.S191: Introduction to Deep Learning\n",
    "- http://introtodeeplearning.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-97SDET3JG-"
   },
   "source": [
    "# Lab 1: Intro to TensorFlow and Music Generation with RNNs\n",
    "\n",
    "# Part 2: Music Generation with RNNs\n",
    "\n",
    "Music generation을 위한 RNN 구축 : ABC notation으로 표현된 악보에서 패턴을 학습하도록 훈련한 후 모델을 이용하여 새로운 음악을 생성하는 것이 과제(task).\n",
    "\n",
    "- ABC noation이란 악보를 A부터 G까지 문자 표기법을 사용하여 나타낸 것을 말합니다. 번호, 제목, 작곡가, 음표, 길이, 음계 등이 포함되어 있고 각 알파벳 별로 의미하는 것이 정해져 있습니다. \n",
    "\n",
    "모델 향상을 위해서 어떻게 해야 할까?\n",
    "*  How does the number of training epochs affect the performance?\n",
    "*  What if you alter or augment the dataset? \n",
    "*  Does the choice of start string significantly affect the result? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsvlBQYCrE4I"
   },
   "source": [
    "## 2.1 Dependencies\n",
    "\n",
    "cousre 저장소 다운로드, 실습 관련 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "riVZCVK65QTH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\hyeongbin\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\hyeongbin\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\hyeongbin\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\hyeongbin\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\hyeongbin\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-gpu (c:\\users\\hyeongbin\\appdata\\roaming\\python\\python38\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mitdeeplearning in c:\\users\\hyeongbin\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hyeongbin\\anaconda3\\lib\\site-packages (from mitdeeplearning) (4.62.3)\n",
      "Requirement already satisfied: gym in c:\\users\\hyeongbin\\anaconda3\\lib\\site-packages (from mitdeeplearning) (0.21.0)\n",
      "Requirement already satisfied: regex in c:\\users\\hyeongbin\\anaconda3\\lib\\site-packages (from mitdeeplearning) (2021.8.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\hyeongbin\\appdata\\roaming\\python\\python38\\site-packages (from mitdeeplearning) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\hyeongbin\\anaconda3\\lib\\site-packages (from gym->mitdeeplearning) (2.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hyeongbin\\anaconda3\\lib\\site-packages (from tqdm->mitdeeplearning) (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "지정된 경로를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# Import Tensorflow 2.0\n",
    "# %tensorflow_version 2.x\n",
    "import tensorflow as tf \n",
    "\n",
    "# Download and import the MIT 6.S191 package\n",
    "!pip install mitdeeplearning\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "# Import all remaining packages\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
    "\n",
    "# Import\n",
    "import random\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "def save_song_to_abc(song, filename=\"tmp\"):\n",
    "    save_name = \"{}.abc\".format(filename)\n",
    "    with open(save_name, \"w\") as f:\n",
    "        f.write(song)\n",
    "    return filename\n",
    "\n",
    "def abc2wav(abc_file):\n",
    "    suf = abc_file.rstrip('.abc')\n",
    "    cmd = \"abc2midi {} -o {}\".format(abc_file, suf + \".mid\")\n",
    "    os.system(cmd)\n",
    "    cmd = \"timidity {}.mid -Ow {}.wav\".format(suf, suf)\n",
    "    return os.system(cmd) \n",
    "\n",
    "def play_wav(wav_file):\n",
    "    return ipythondisplay.Audio(wav_file)\n",
    "\n",
    "def play_song(song):\n",
    "    basename = save_song_to_abc(song)\n",
    "    ret = abc2wav(basename + '.abc')\n",
    "    if ret == 0: #did not suceed\n",
    "        return play_wav(basename+'.wav')\n",
    "    return None\n",
    "\n",
    "# Check that we are using a GPU, if not switch runtimes\n",
    "#   using Runtime > Change Runtime Type > GPU\n",
    "#assert len(tf.config.list_physical_devices('GPU')) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ajvp0No4qDm"
   },
   "source": [
    "## 2.2 Dataset\n",
    "\n",
    "817 songs, 200,679 vectorized songs\n",
    "\n",
    "- ABC notation으로 표현된 수천 개의 아일랜드 민요(Irish folk songs) 데이터 \n",
    "- 단순히 연주되는 음에 대한 정보만이 아니라 추가적으로 노래 제목, 키, 템포와 같은 메타 정보도 포함함 \n",
    "- **텍스트 파일에 존재하는 다양한 문자들이 complexity of the learning problem에 어떤 영향을 미칠까?**\n",
    "- 위 문제는 텍스트 데이터를 수치 데이터로 만들때 중요해짐 : Preprocessing\n",
    "- 음악데이터만 사용하려면 어떻게 해야 할까요? \n",
    "- NLP 적인 처리가 필요하지 않을까? \n",
    "    - X : number\n",
    "    - T : title, 제목\n",
    "    - Z : ?? 음자리표인감 높은음자리표 이런거.. \n",
    "    - M : 뭐지? 첫 시작음? \n",
    "    - L : 박자표인듯\n",
    "    - K : key, 음계, 다장조 이런거 \n",
    "    - z : 악보, 우리가 실제로 훈련해야 할 것, 여기서 | 이거는 마디, !는 뭔지 모르겠음 :| 도돌이표\n",
    "- **songs** : 817개의 노래가 들어있는 list\n",
    "- **show_music_detail(num)** : num 개의 음악을 자세하게 볼 수 있습니다 \n",
    "- **mdl.lab1.play_song(example_song)** : text형 음악을(ABC notation) audio 파형으로 표현합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 817 songs in text\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "# colab으로 실행한다면 주석과 같이 작성할 것 \n",
    "# jupyter notebook으로 실행한다면 아래와 같이 작성할 것 \n",
    "# import pattern과 관련해서 수정해주어야 오류가 해결되는 것 같습니다\n",
    "\n",
    "from mitdeeplearning import lab1\n",
    "#songs = mdl.lab1.load_training_data()\n",
    "songs = lab1.load_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example song # 322 :\n",
      "X:44\n",
      "T:Toormore\n",
      "Z: id:dc-polka-35\n",
      "M:2/4\n",
      "L:1/8\n",
      "K:D Major\n",
      "z|ef/e/ dB|BA B/c/d|ef/e/ dB|BA A2|!\n",
      "ef/e/ dB|BA B/c/d|ab/a/ gc|ed d:|!\n",
      "e|f>e fA|B/c/d ef|g>f ga|ba f/g/a|!\n",
      "f>e fA|B/c/d ef|ab/a/ gc|ed d:|!\n",
      "\n",
      "Example song # 735 :\n",
      "X:17\n",
      "T:Lodge Road\n",
      "Z: id:dc-setdance-18\n",
      "M:C|\n",
      "L:1/8\n",
      "K:D Major\n",
      "A2|d2df edcB|A2A2 A2F2|DFAF DFAF|G2FG E2A2|!\n",
      "d2df edcB|A2A2 A2F2|DFAF GFEF|D2DC D2:|!\n",
      "A2|defg agfe|defg a2af|g2eg f2df|edcB ABcA|!\n",
      "f2f2 g2g2|e2e2 f2fe|dfaf dfaf|g2fg e2e2|!\n",
      "f2f2 g2g2|e2e2 f2fe|dfaf bgec|d2dc d2|]!\n",
      "\n",
      "Example song # 178 :\n",
      "X:110\n",
      "T:Old Leitrim\n",
      "Z: id:dc-jig-91\n",
      "M:6/8\n",
      "L:1/8\n",
      "K:G Major\n",
      "D|G2G AGA|BGE EDE|GBd egd|egd edB|!\n",
      "G2G AGA|BGE EDE|GBd ege|dBA G2:|!\n",
      "D|GBd egd|egd edB|GBd ded|ded dBA|!\n",
      "[1 GBd egd|egd edB|GBd ege|dBA G2:|!\n",
      "[2 G2G AGA|BGE EDE|GBd ege|dBA G2|]!\n"
     ]
    }
   ],
   "source": [
    "# Print one of the songs to instpect it in greater detail! \n",
    "# my function -> Print N of the song... \n",
    "def show_music_detail(songs, num) :\n",
    "    num_list = sample(range(0, len(songs)), num) #sample output format is list\n",
    "    for i in num_list :\n",
    "        example_song = songs[i]\n",
    "        print(\"\\nExample song #\", i, \":\")\n",
    "        print(example_song)\n",
    "    \n",
    "show_music_detail(songs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_song = songs[random.randint(0, len(songs))]\n",
    "mdl.lab1.play_song(example_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## 2.3 Preprocessing the dataset\n",
    "\n",
    "- Vectorize the text \n",
    "    - 텍스트(text) 데이터셋 -> 수치(numerical) 데이터셋 => Vectorize \n",
    "    - **vocab**     : ABC notation에 있는 unique vocabulary(or character) \n",
    "    - **char2idx** : 문자 -> 숫자 mapping, 정수  \n",
    "    - **idx2char** : 숫자 -> 문자 mapping \n",
    "    - **vectorized_songs** : ABC notation 노래 text에서 벡터화된 노래 \n",
    "- Make input sequence chunk from input text (and target sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IalZLbvOzf-F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 83 unique characters in the dataset\n",
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '\"' :   3,\n",
      "  '#' :   4,\n",
      "  \"'\" :   5,\n",
      "  '(' :   6,\n",
      "  ')' :   7,\n",
      "  ',' :   8,\n",
      "  '-' :   9,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Join our list of song strings into a single string containing all songs\n",
    "# 노래 문자열(string) 목록을 이 모든 노래가 포함된 하나의 단일 문자열로 결합함\n",
    "songs_joined = \"\\n\\n\".join(songs) \n",
    "\n",
    "# Find all unique characters in the joined string\n",
    "# 결합된 노래 묶음 문자열에서 유일한(unique) 단어들을 모두 확인\n",
    "# 데이터셋에는 총 83개의 유일한 문자들이 존재함\n",
    "vocab = sorted(set(songs_joined))\n",
    "print(\"There are\", len(vocab), \"unique characters in the dataset\")\n",
    "\n",
    "\n",
    "### Define numerical representation of text ###  \n",
    "# 문자 -> unique index로 매핑\n",
    "# 예를 들어, \"d\"라는 문자의 인덱스는 char2idx[\"d\"]와 같이 작성하여 얻을 것임 \n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "\n",
    "# index -> 문자로 매핑 \n",
    "# char2idx의 반대 버전이며 index에서 다시 문자로 되돌리는 역할을 할 것임\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# [python] difference between str() and repr() \n",
    "# 숫자를 문자열로 변환시켜주는 함수임은 동일합니다. 하지만 기저에 작동하는 원리가 다른 것 같습니다. \n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(10)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200679\n",
      "\n",
      "\"X:1\\nT:Alexander's\\nZ:\" \n",
      "---- characters mapped to int ---->\n",
      " [49 22 13  0 45 22 26 67 60 79 56 69 59 60 73  5 74  0 51 22]\n"
     ]
    }
   ],
   "source": [
    "### Vectorize the songs string ###\n",
    "# 문자열 벡터화 함수, return은 무조건 'N(입력 문자열의 문자 개수)'요소의 np.array\n",
    "def vectorize_string(string):\n",
    "    # TODO\n",
    "    vectorized_list = []\n",
    "    vectorized_list = ([char2idx[_] for _ in string])\n",
    "    return np.array(vectorized_list)\n",
    "\n",
    "# my function -> Print N length of vectorized songs \n",
    "# 나중에 곡 하나를 통째로 vectorized 한 결과를 두 개 (원곡, vectorized) 보여주는 함수도 짜면 좋을 것 같음 \n",
    "def show_music_vectorized(vectorized_songs, length) :\n",
    "    print ('\\n{} \\n---- characters mapped to int ---->\\n {}'.format(repr(songs_joined[:length]), vectorized_songs[:length]))\n",
    "\n",
    "vectorized_songs = vectorize_string(songs_joined)\n",
    "print(len(vectorized_songs)) # 200,679\n",
    "show_music_vectorized(vectorized_songs, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets\n",
    "\n",
    "- 실제 텍스트를 훈련 중에 사용할 예제 시퀀스로 나눌 것\n",
    "- 입력 텍스트를 seq_length 단위로 나눔\n",
    "- 예를 들어 텍스트는 Hello 이고 seq_length가 4라면 hell가 input, elloo가 target이 됨 \n",
    "- 이로부터 batch 방법을 사용하여 해당 문자 인덱스 스트림을 원하는 크기의 시퀀스로 변환 가능 \n",
    "- 각각의 벡터,각각의 index는 하나의 단일 time step으로 처리 \n",
    "- 즉, time step 0 은 들어온 시퀀스의 첫 번째 문자에 대한 인덱스를 받고 그 다음 인덱스 1의 문자를 맞출 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LF-N8F7BoDRi"
   },
   "outputs": [],
   "source": [
    "### Batch definition to create training examples ###\n",
    "def get_batch(vectorized_songs, seq_length, batch_size):\n",
    "    # the length of the vectorized songs string\n",
    "    n = vectorized_songs.shape[0] - 1 # 200679 - 1 = 200678 \n",
    "    # randomly choose the starting indices for the examples in the training batch\n",
    "    # 0 ~ 200678-seq_length 숫자 사이에서 batch_size개의 임의의 표본 추출 -> 즉 batch size가 n개면 idx도 n개 나옴\n",
    "    idx = np.random.choice(n-seq_length, batch_size) \n",
    "    #print(\"idx: \", idx) \n",
    "    \n",
    "    '''TODO: construct a list of input sequences for the training batch'''\n",
    "    input_batch = [vectorized_songs[i : i+seq_length] for i in idx]\n",
    "    #print(\"input batch: \", len(input_batch)) # batch size 는 seq_length 값과 동일, 그래서 step의 수도 0~seq_length-1 까지임! \n",
    "    '''TODO: construct a list of output sequences for the training batch'''\n",
    "    output_batch = [vectorized_songs[i+1 : i+seq_length+1] for i in idx]\n",
    "    #print(\"output batch: \", len(output_batch))\n",
    "    \n",
    "    # x_batch, y_batch provide the true inputs and targets for network training\n",
    "    x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
    "    y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0eBu9WZG84i0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0\n",
      "  input: 82 ('|')\n",
      "  expected output: 59 ('d')\n",
      "Step   1\n",
      "  input: 59 ('d')\n",
      "  expected output: 14 ('2')\n",
      "Step   2\n",
      "  input: 14 ('2')\n",
      "  expected output: 27 ('B')\n",
      "Step   3\n",
      "  input: 27 ('B')\n",
      "  expected output: 1 (' ')\n",
      "Step   4\n",
      "  input: 1 (' ')\n",
      "  expected output: 58 ('c')\n"
     ]
    }
   ],
   "source": [
    "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
    "\n",
    "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
    "    print(\"Step {:3d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='index'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def graph(pred) :\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(pred, label = 'actual')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#graph(vectorized_songs)\n",
    "df2 = pd.DataFrame(vectorized_songs, columns = [\"songs\"])\n",
    "df2.reset_index().plot(x='index', y='songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>songs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200674</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200675</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200676</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200677</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200678</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200679 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        songs\n",
       "0          49\n",
       "1          22\n",
       "2          13\n",
       "3           0\n",
       "4          45\n",
       "...       ...\n",
       "200674     32\n",
       "200675     15\n",
       "200676     22\n",
       "200677     82\n",
       "200678      2\n",
       "\n",
       "[200679 rows x 1 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 2.4 The Recurrent Neural Network (RNN) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    " 모델은 LSTM 구조를 기반하는데, 이 구조는 연속되는 문자들 사이의 시간 관계(temporal relationships)에 대한 정보를 유지하는 state 벡터를 사용합니다. LSTM의 최종 출력은 fully-connected `Dense` layer로 넘겨지는데, 여기서 vocabulary의 각 문자에 대해 softmax를 계산하고(출력하고) 이로부터 만들어진 불포에서 샘플을 추춘하여 다음 문자를 예측합니다. \n",
    "\n",
    "* keras API [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential)를 사용합니다. 모델을 정의하기 위해 세 개의 레이어가 사용됩니다. \n",
    "* [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) : 입력 레이어, 각 문자의 숫자들을 `embedding_dim`차원의 벡터로 매핑하는 훈련 가능한 lookup 테이블로 구성됨 \n",
    "* [`tf.keras.layers.LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): `units=rnn_units` 크기의 LSTM 신경망\n",
    "* [`tf.keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): `vocab_size` 출력이 있는 출력 레이어\n",
    "* `Model.summary` : 모델의 내부 작동에 대한 summary를 출력, 모델 내부의 레이어, 각 레이어의 출력 형태(shape), 배치 사이즈 등을 확인해 볼 수 있습니다.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_unrolled-01-01.png\" alt=\"Drawing\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8DsWzojvkbc7"
   },
   "outputs": [],
   "source": [
    "# define model function \n",
    "def LSTM(rnn_units): \n",
    "    return tf.keras.layers.LSTM(\n",
    "        rnn_units, \n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        recurrent_activation='sigmoid',\n",
    "        stateful=True,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "### Defining the RNN Model ###\n",
    "'''TODO: Add LSTM and Dense layers to define the RNN model using the Sequential API.'''\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Layer 1: Embedding layer to transform indices into dense vectors of a fixed embedding size\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "\n",
    "        # Layer 2: LSTM with `rnn_units` number of units. \n",
    "        # TODO: Call the LSTM function defined above to add this layer.\n",
    "        LSTM(rnn_units),\n",
    "\n",
    "        # Layer 3: Dense (fully-connected) layer that transforms the LSTM output into the vocabulary size. \n",
    "        # TODO: Add the Dense layer.\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "# Build a simple model with default hyperparameters. You will get the chance to change these later.\n",
    "model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RwG1DD6rDrRM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           21248     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 83)            85075     \n",
      "=================================================================\n",
      "Total params: 5,353,299\n",
      "Trainable params: 5,353,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "### Test out the RNN model\n",
    "\n",
    "모델의 예상 작동을 확인하기 위한 간단한 테스트 시행합니다. 길이 100의 시퀀스를 사용하여 출력물의 차원도 확인해보자. \n",
    "**모델에는 어떤 길이의 입력도 동작할 수 있음을 주의!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  [120859  50113  65618  57080  83245 174192  92495 175885  63312  77416\n",
      "  61254  92585  11071 129007  85290 196486 177923 179188 182693  97211\n",
      "  64206  81520 195326  92560  77703 150613 191106   2144  27006 122422\n",
      "  65328 112407]\n",
      "input batch:  32\n",
      "output batch:  32\n",
      "Input shape:       (32, 100)  # (batch_size, sequence_length)\n",
      "Prediction shape:  (32, 100, 83) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
    "pred = model(x)\n",
    "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
    "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT1HvFVUGpoE"
   },
   "source": [
    "### Predictions from the untrained model\n",
    "\n",
    "훈련시키지 않은 모델의 예측은 어떨까? \n",
    "\n",
    "모델의 실제 예측을 얻기 위해서, 우리는 문자 vocabulary를 `softmax`로 정의한 출력 분포에서 샘플을 추출해야 합니다. 이는 우리에게 실제 문자 인덱스틀 제공합니다. 즉, 범주형 분포([categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) )을 사용하여 예제 예측을 샘플로 추출함을 의미합니다. 이로써 각 time step에서 다음 문자(특히나 그 인덱스에서의)에 대한 예측을 하게 됩니다. \n",
    "\n",
    "여기서는 단순히 `argmax`를 취하는 대신 확률 분포에서 표본을 추출합니다. (그리고 argmax는 모델이 루프에 빠지게 만들 수 있습니다?)\n",
    "\n",
    "첫 번째 예시 batch에서 샘플링을 실행해봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55, 51, 60, 16, 81, 73, 26,  0, 39, 51, 82, 24, 26, 62, 38, 42,  7,\n",
       "       80, 40, 71, 63, 26, 80,  1, 76, 41, 17, 18, 48, 45,  6, 33, 19, 53,\n",
       "       77, 65, 70, 18, 14, 58, 70, 14, 32, 82, 75, 46, 73,  2, 19, 38, 52,\n",
       "       10, 80, 35, 30,  4,  1, 17,  4, 59, 34, 77,  8, 69, 10, 76, 67, 66,\n",
       "       16, 79, 79,  2, 43, 59, 42, 77, 34, 46, 49,  0, 24, 31,  0, 23, 82,\n",
       "       64, 33, 30, 46, 27, 80, 45, 12, 78, 63, 31, 58, 80,  2,  3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이들을 디코딩하여 훈련되지 않은 모델이 예측한 텍스트를 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'D2:|!\\n\\nX:160\\nT:Jenny Rocking the Cradle\\nZ: id:dc-reel-147\\nM:C\\nL:1/8\\nK:D Major\\nFE|DFFd AFFA|DFFd ABAF'\n",
      "\n",
      "Next Char Predictions: \n",
      " '_Ze4zrA\\nNZ|=AgMQ)yOphAy uP56WT(H7]vjo62co2G|tUr!7M[.yJE# 5#dIv,n.ulk4xx!RdQvIUX\\n=F\\n<|iHEUByT0whFcy!\"'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## 2.5 Training the model: loss and training operations\n",
    "\n",
    "다음 문자를 예측하는 문제를 일반적으로 분류 문제(classification, 여기서는 이진 분류가 아닌 multi-class)로 생각해 볼 수 있습니다. \n",
    "RNN의 이전 state와 주어진 time step에서의 입력을 고려하여 다음 문자에 대한 class를 예측해야 합니다.\n",
    "\n",
    "### loss \n",
    "해당 분류 작업에 대해 모델을 훈련시키기 위해 `crossentropy` loss 형태를 사용할 수 있습니다. 특히나, `sparse_categorical_crossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/sparse_categorical_crossentropy) loss를 사용할 것인데, 해당 loss는 범주형 분류 작업을 위해 정수 타겟을 사용하기 때문입니다. 실제 타겟(`labels`)과 예측 타겟(`logits`)을 사용하여 loss를 계산하는 방식입니다.  \n",
    "\n",
    "### hyper parameter tuning \n",
    "시작값 제공, 최적의 파라미터는 여러분들이 찾아보세요~\n",
    "\n",
    "### optimizer and training operation \n",
    "optimizer와 epochs은 신경망 출력에 영향을 줄 수 있음 \n",
    "- 아래는 시도해볼만 한 optimizer\n",
    "- [`Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam?version=stable)\n",
    "- [`Adagrad`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad?version=stable) \n",
    "\n",
    "Backpropagation은 [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)를 사용 \n",
    "\n",
    "훈련 동안 모델의 진행 상황을 출력하여 loss를 최소화하고 있는지에 대한 여부를 시각화함\n",
    "\n",
    "### Benchmarking \n",
    "1. No-trained model -> scalar loss : 4.418331 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "4HrXTACTdzY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 100, 83)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.419138\n"
     ]
    }
   ],
   "source": [
    "### Defining the loss function ###\n",
    "'''TODO: define the loss function to compute and return the loss between\n",
    "    the true labels and predictions (logits). Set the argument from_logits=True.'''\n",
    "def compute_loss(labels, logits):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True) # TODO\n",
    "    return loss\n",
    "\n",
    "'''TODO: compute the loss using the true next characters from the example batch \n",
    "and the predictions from the untrained model several cells above'''\n",
    "example_batch_loss = compute_loss(y, pred)\n",
    "\n",
    "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "JQWUUhKotkAY"
   },
   "outputs": [],
   "source": [
    "### Hyperparameter setting and optimization ###\n",
    "\n",
    "# Optimization parameters:\n",
    "num_training_iterations = 2000  # Increase this to train longer\n",
    "batch_size = 4  # Experiment between 1 and 64\n",
    "seq_length = 100  # Experiment between 50 and 500\n",
    "learning_rate = 5e-3  # Experiment between 1e-5 and 1e-1\n",
    "\n",
    "# Model parameters: \n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256 \n",
    "rnn_units = 1024  # Experiment between 1 and 2048\n",
    "\n",
    "# Checkpoint location: \n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "F31vzJ_u66cb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                               | 1/2000 [00:04<2:15:16,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  [61119 61550  8667 87169]\n",
      "input batch:  4\n",
      "output batch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 1/2000 [00:05<3:12:48,  5.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HYEONG~1\\AppData\\Local\\Temp/ipykernel_7704/4049686193.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Grab a batch and propagate it through the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorized_songs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Update the progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Define optimizer and training operation ###\n",
    "\n",
    "'''TODO: instantiate a new model for training using the `build_model`\n",
    "  function and the hyperparameters created above.'''\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
    "\n",
    "'''TODO: instantiate an optimizer with its learning rate.\n",
    "  Checkout the tensorflow website for a list of supported optimizers.\n",
    "  https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/\n",
    "  Try using the Adam optimizer to start.'''\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y): \n",
    "    # Use tf.GradientTape()\n",
    "    with tf.GradientTape() as tape:\n",
    "        '''TODO: feed the current input into the model and generate predictions'''\n",
    "        y_hat = model(x)\n",
    "  \n",
    "        '''TODO: compute the loss!'''\n",
    "        loss = compute_loss(y, y_hat)\n",
    "\n",
    "    # Now, compute the gradients \n",
    "    '''TODO: complete the function call for gradient computation. \n",
    "      Remember that we want the gradient of the loss with respect all \n",
    "      of the model parameters. \n",
    "      HINT: use `model.trainable_variables` to get a list of all model\n",
    "      parameters.'''\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "  \n",
    "    # Apply the gradients to the optimizer so it can update the model accordingly\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "##################\n",
    "# Begin training!#\n",
    "##################\n",
    "\n",
    "history = []\n",
    "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
    "if hasattr(tqdm, '_instances'): tqdm._instances.clear() # clear if it exists\n",
    "\n",
    "for iter in tqdm(range(num_training_iterations)):\n",
    "    # Grab a batch and propagate it through the network\n",
    "    x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
    "    loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # Update the progress bar\n",
    "    history.append(loss.numpy().mean())\n",
    "    plotter.plot(history)\n",
    "\n",
    "    # Update the model with the changed weights!\n",
    "    if iter % 100 == 0:     \n",
    "        model.save_weights(checkpoint_prefix)\n",
    "    \n",
    "    # Save the trained model and the weights\n",
    "    model.save_weights(checkpoint_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 2.6 Generate music using the RNN model : Inference\n",
    "\n",
    "음악을 만들어낼 때, 모델의 시작을 위해 몇 가지 시드를 공급해야 합니다. \n",
    "\n",
    "일단 생성된 시드를 가지고 나면, 훈련된 RNN을 사용하여 각각의 연속적인 문자를 반복적으로 예측할 수 있습니다. 보다 구체적으로, RNN은 가능한 연속적인 문자로 `softmax`를 출력한다는 것을 기억하자. 추론(Inference)을 하는 동안, 이런 분포로부터 반복적으로 표본을 추출한 다음 생성된 노래를 ABC 표기법으로 인코딩하는 데 샘플을 사용합니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### Restore the latest checkpoint\n",
    "\n",
    "- 추론(Inference) 단계를 간단하게 하기 위해서 공정 batch 크기는 1로 사용\n",
    "    - RNN의 state가 timestep에서 timestep으로 전달되는 방식 때문에, 모델은 오로지 한번 설정된 고정 batch 크기만을 수용할 수 있습니다. \n",
    "    - 다른 batch_size 모델을 실행하려면 rebuilding 해야 하고 훈련 중 마지막 checkpoint 이후의 가중치를 복원해야 함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (1, None, 256)            21248     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, None, 83)             85075     \n",
      "=================================================================\n",
      "Total params: 5,353,299\n",
      "Trainable params: 5,353,299\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''TODO: Rebuild the model using a batch_size=1'''\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Restore the model weights for the last checkpoint after training\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prediction procedure\n",
    "\n",
    "- \"시드\" 시작 문자열과 RNN state를 초기화 하고, 생성할 문자 수를 설정합니다.\n",
    "- 시작 문자열과 RNN state를 사용하여 다음 예측 문자에 대한 확률 분포를 얻습니다. \n",
    "- 예측 문자의 인덱스를 계산하기 위해 다항 분포 표본으로부터 샘플링합니다. 그런 다음 이 예측 문자를 모델의 다음 입력으로 사용합니다. \n",
    "- 각 time step에서, 업데이트 된 RNN state는 다시 모델에 공급되어, 다음 예측을 만들 때 더 많은 컨텍스트(context)를 갖게 됩니다. 다음 문자를 예측한 후, 업데이트 된 RNN states는 다시 모델에 공급되고, 이는 이전 예측으로부터 더 많은 정보를 얻기 때문에 데이터 속에서 시퀀스 종속성을 학습하게 하는 방법이 됩니다. \n",
    "\n",
    "![LSTM inference](https://raw.githubusercontent.com/aamini/introtodeeplearning/2019/lab1/img/lstm_inference.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "### Prediction of a generated song ###\n",
    "\n",
    "def generate_text(model, start_string, generation_length=1000):\n",
    "    # Evaluation step (generating ABC text using the learned RNN model)\n",
    "\n",
    "    '''TODO: convert the start string to numbers (vectorize)'''\n",
    "    #input_eval = [vectorize_string(start_string)]\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    tqdm._instances.clear()\n",
    "\n",
    "    for i in tqdm(range(generation_length)):\n",
    "        '''TODO: evaluate the inputs and generate the next character predictions'''\n",
    "        predictions = model(input_eval)\n",
    "      \n",
    "        # Remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "      \n",
    "        '''TODO: use a multinomial distribution to sample'''\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "        # Pass the prediction along with the previous hidden state\n",
    "        #   as the next inputs to the model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "        '''TODO: add the predicted character to the generated text!'''\n",
    "        # Hint: consider what format the prediction is in vs. the output\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 197.26it/s]\n"
     ]
    }
   ],
   "source": [
    "'''TODO: Use the model and the function defined above to generate ABC format text of length 1000!\n",
    "    As you may notice, ABC files start with \"X\" - this may be a good start string.'''\n",
    "generated_text = generate_text(model, start_string=\"X\", generation_length=1000) # TODO\n",
    "# generated_text = generate_text('''TODO''', start_string=\"X\", generation_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "### Play back the generated music!\n",
    "\n",
    "ABC notation text audio -> audio file 로 변환 후 생성된 노래 확인  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "LrOtG64bfLto"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 songs in text\n"
     ]
    }
   ],
   "source": [
    "### Play back generated songs ###\n",
    "\n",
    "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
    "\n",
    "for i, song in enumerate(generated_songs): \n",
    "    # Synthesize the waveform from a song\n",
    "    waveform = mdl.lab1.play_song(song)\n",
    "\n",
    "    # If its a valid song (correct syntax), lets play it! \n",
    "    if waveform:\n",
    "        print(\"Generated song\", i)\n",
    "        ipythondisplay.display(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "uoJsVjtCMunI"
   ],
   "name": "Part2_Music_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
